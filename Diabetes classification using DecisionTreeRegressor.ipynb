{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELjbdaHdKShu"
   },
   "source": [
    "Gradient boosting is another boosting algorithm. It is more generalized boosting framework compared to AdaBoosting, which also makes it more complicated and math-intensive. Instead of trying to emphasize problematic instances by assigning weights and resampling the dataset, gradient boosting builds each base learner's errors. Furthermore, gradient boositng uses decision trees of varying depths. In this video , we will present gradient boosting without delving much into math involved. Instead we will present the basic concept and also implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZlfZUOWK9s1"
   },
   "source": [
    "**Creating the ensemble**\n",
    "\n",
    "Following that, it creates a decision tree that tries to predict the pseudo-residuals. By repeating this process, a number of times, the whole ensemble is created. Similar to AdaBoost, gradient boosting assigns a weight to each tree. Contrary to AdaBoost, this weight does not depend on the tree's performance. Instead it is a constant term, which is called learning rate. And its purpose is to increase the ensemble's generalization ability , by restricting its over-fitting power. There are 9 steps on this algorithm:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1.   Defining the learning rate (smaller than 1 ) and the ensemble's size\n",
    "\n",
    "\n",
    "\n",
    "1.   Calculate the train set's target mean\n",
    "\n",
    "\n",
    "\n",
    "1.   Using the mean as a very simple initial prediction, calculate each instance's target difference from the mean. These errors are called pseudo-residuals\n",
    "\n",
    "\n",
    "\n",
    "1.   Build a decision tree, by using the original train set's features and the pseudo-residulas as targets\n",
    "\n",
    "\n",
    "\n",
    "1.   Make predictions on the train set, using the decision tree (we try to predict the pseudo-residuals)\n",
    "\n",
    "\n",
    "\n",
    "1.   Multiply the predicted values by the learning rate\n",
    "\n",
    "2.   Add the multiplied values to the previously stored predicted values. Use the newly calculated values as predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.   Calculate the new pseudo-residuals using the calculated predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.   Repeat from step 4 until the desired ensemble size is achieved.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note that in order to produce the final ensemble's predictions, each base learner's predictions is multiplied by the learning rate and added to the previous learner's prediction. The calculated mean can be regarded as the first base learner's prediction.\n",
    "\n",
    "\n",
    "\n",
    "At each step s for a learning rate lr, the prediction is calculated by this formula: ps = mean +lr.p1+lr.p2+....+lr*ps\n",
    "\n",
    "\n",
    "\n",
    "The residuals care calculated as the difference from the actual target value t:\n",
    "\n",
    "r2 = t-ps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n7pWT5Z9Ite7"
   },
   "outputs": [],
   "source": [
    "# Import the linbraries and data\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "train_size = 400\n",
    "#Split the train and test set\n",
    "train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]\n",
    "test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgP_PyVaOuLb"
   },
   "source": [
    "Following this, we define the ensemble's size, learning rate and Decision Tree's maximum depth. Furthermore, we will create a list to store individual base learners, as well as Numpy array to store the previous predictions.\n",
    "\n",
    "As mentioned eariler, our initial prediction is the train set's target mean. Insread of defining a maximum depth, we could also define a maximum num ber of leaf nodes by passing the max_leaf_nodes = 3 argument to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vHq9pKDfPMNx"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the ensemble and define the ensemble's size, learning rate and decision tree depth\n",
    "ensemble_size = 50\n",
    "learning_rate = 0.1\n",
    "base_classifier = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Create placeholders for the base learners and each step's prediction\n",
    "base_learners = []\n",
    "# Note that the initial prediction is the target variable's mean\n",
    "previous_predictions = np.zeros(len(train_y)) + np.mean(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtfYRYkUPx2G"
   },
   "source": [
    "The next step is to create and train the ensemble. We start by calculating the pseudo-residuals using the previous predictions. We then create a deep copy of the base learner class and train it on the ttrain set using the pseudo-residuals as targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_khP5RjyQERt"
   },
   "outputs": [],
   "source": [
    "# Crteate the base learners\n",
    "for _ in range(ensemble_size):\n",
    "    # Start by calcualting the pseudo-residuals\n",
    "    errors = train_y - previous_predictions\n",
    "\n",
    "    # Make a deep copy of the base classifier and train it on the\n",
    "    # pseudo-residuals\n",
    "    learner = deepcopy(base_classifier)\n",
    "    learner.fit(train_x, errors)\n",
    "\n",
    "    # Predict the residuals on the train set\n",
    "    predictions = learner.predict(train_x)\n",
    "    # Multiply the predictions witht he learning rate and add the results to the previous prediction\n",
    "    previous_predictions = previous_predictions + learning_rate* predictions\n",
    "    # Save the base learner\n",
    "    base_learners.append(learner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq24PS3RQzfx"
   },
   "source": [
    "Finally we use the trained base learner in order to predict the pseudo-residuals on the train set. We multiply the predictions by the learning rate and add them to the previous predictions. Finally, we append the base learner to the base_learners list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_6ZagZ5R-2Y"
   },
   "source": [
    "In order to make predictions with our ensemble and evaluate it, we use the test set's features  on order to predict pseudo-residuals, multiply them by the learning rate and dd them to the trainset's target mean. It is important to use th eoriginal train set's mean as a starting point because each tree predicts the deviations from that orginal mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h7OXnpuYRq1c"
   },
   "outputs": [],
   "source": [
    "# Step 3: Evaluate the ensemble\n",
    "# Start with the train set's mean\n",
    "previous_predictions = np.zeros(len(test_y)) + np.mean(train_y)\n",
    "# for each base learner predict the pseudo-residuals for the test set and add them to the previous prediction, after multiplying with the learning rate\n",
    "for learner in base_learners:\n",
    "  predictions= learner.predict(test_x)\n",
    "  previous_predictions = previous_predictions+ learning_rate*predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Xh4JZadS6Ie",
    "outputId": "e9f2ce3a-473e-45c4-d49a-3786ad70c274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Bossting: \n",
      "R-squred: 0.59 \n",
      "MSE: 2253.34\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Print the metrics\n",
    "r2 = metrics.r2_score(test_y, previous_predictions)\n",
    "mse = metrics.mean_squared_error(test_y, previous_predictions)\n",
    "print('Gradient Bossting: ')\n",
    "print('R-squred: %.2f '%r2)\n",
    "print('MSE: %.2f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B0aTEtnTWC3"
   },
   "source": [
    "The algorithm is able to achieve an R-squred value 0f 0.59 and an MSE of 2255.44 with this particular setup."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Udemy Gradient Boosting Introduction and implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
