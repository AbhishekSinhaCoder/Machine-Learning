{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KF9kDKwqINLN"
   },
   "outputs": [],
   "source": [
    "#Step 1: Import libraries and data\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WEScLKlsKx8f"
   },
   "outputs": [],
   "source": [
    "# Split the traing set and testing set\n",
    "train_x, train_y = diabetes.data[:400], diabetes.target[:400]\n",
    "test_x, test_y = diabetes.data[400:], diabetes.target[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cuV-l0tmLByI"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the ensemble's base learners and meta-learner and then append base learners to a list for ease of access\n",
    "base_learners =[]\n",
    "knn =  KNeighborsRegressor(n_neighbors=5)\n",
    "base_learners.append(knn)\n",
    "dtr = DecisionTreeRegressor(max_depth=4, random_state=123456)\n",
    "base_learners.append(dtr)\n",
    "ridge = Ridge()\n",
    "base_learners.append(ridge)\n",
    "meta_learner = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHvjAjT7L7ro"
   },
   "source": [
    "After instantiating our learners, we need to create the metadata for the training set. We split the training set into 5 folds by first creating a KFold object, specifying the number of splits(K) with KFold(n_splits=5), and then calling KF.split(train_x).\n",
    "\n",
    "\n",
    "This, in turn, returns a generator for the train and test indices of the five splits. For each of these splits, we use the data indicated by train_indices (four folds) to train our base learners and create metadata on the data corresponding to test_indices. \n",
    "\n",
    "Furthermore, we will store the metadata for each classifier in the meta_data array and the corressponding targets in the meta_targets array.\n",
    "\n",
    "Finally we transpose meta_data in order to get a (instance, feature) shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eIK1uSdaMzQ6"
   },
   "outputs": [],
   "source": [
    "# Step 3: Creating the training metadata\n",
    "# Creating variables to store metadata and their targets\n",
    "meta_data = np.zeros((len(base_learners), len(train_x)))\n",
    "meta_targets = np.zeros(len(train_x))\n",
    "\n",
    "# Create the corss-validation folds\n",
    "\n",
    "KF = KFold(n_splits=5)\n",
    "meta_index = 0\n",
    "for train_indices, test_indices in KF.split(train_x):\n",
    "  # Train each learner on the K-1 folds \n",
    "  # and create metadata for the Kth fold\n",
    "  for i in range(len(base_learners)):\n",
    "    learner = base_learners[i]\n",
    "    learner.fit(train_x[train_indices], train_y[train_indices])\n",
    "    predictions = learner.predict(train_x[test_indices])\n",
    "    meta_data[i][meta_index:meta_index+len(test_indices)] = \\\n",
    "                              predictions\n",
    "\n",
    "  meta_targets[meta_index:meta_index+len(test_indices)] = \\\n",
    "                          train_y[test_indices]\n",
    "  meta_index += len(test_indices)\n",
    "\n",
    "# Transpose the metadata to be fed into the meta-learner\n",
    "meta_data = meta_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRfZ6PK1One5"
   },
   "source": [
    "For the test set, we do not need to split it into folds. We simply train the base learners on the whole train set and predict on the test set.\n",
    "\n",
    "\n",
    "Furthermore, we will evaliuate each base learner and store the evaluation metrics, in order to compare them with the ensemble's performance. As this is a regression problem , we will use R-squared and Mean Squared Error (MSE) as evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yO4vvQ2TPGjQ"
   },
   "outputs": [],
   "source": [
    "# Step 4: Cteate the metadata for the test set and evaluate the base learners\n",
    "test_meta_data = np.zeros((len(base_learners), len(test_x)))\n",
    "base_errors = []\n",
    "base_r2 = []\n",
    "for i in range(len(base_learners)):\n",
    "  learner = base_learners[i]\n",
    "  learner.fit(train_x, train_y)\n",
    "  predictions = learner.predict(test_x)\n",
    "  test_meta_data[i] = predictions\n",
    "\n",
    "  err = metrics.mean_squared_error(test_y, predictions)\n",
    "  r2 = metrics.r2_score(test_y, predictions)\n",
    "\n",
    "  base_errors.append(err)\n",
    "  base_r2.append(r2)\n",
    "\n",
    "test_meta_data = test_meta_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CPtY1sFQCEr"
   },
   "source": [
    "Now, we have the metadata for both train and test sets. Now we can train our meta-learner on the train set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u1ty1iU5QOle"
   },
   "outputs": [],
   "source": [
    "# Step 5: We need to fit the meta-learner on the train set and evaluate it on the test set\n",
    "\n",
    "meta_learner.fit(meta_data,meta_targets)\n",
    "ensemble_predictions = meta_learner.predict(test_meta_data)\n",
    "err = metrics.mean_squared_error(test_y,ensemble_predictions)\n",
    "r2 = metrics.r2_score(test_y, ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-NStF0sQqTE",
    "outputId": "7bff4d73-4372-40db-db4b-696453efb95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR R2 Name\n",
      "--------------------\n",
      "2697.8 0.51 KNeighborsRegressor\n",
      "3142.5 0.43 DecisionTreeRegressor\n",
      "2564.8 0.54 Ridge\n",
      "2066.6 0.63 Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Print the results\n",
    "print('ERROR R2 Name')\n",
    "print('-'*20)\n",
    "for i in range(len(base_learners)):\n",
    "  learner = base_learners[i]\n",
    "  print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} {learner.__class__.__name__}')\n",
    "print(f'{err:.1f} {r2:.2f} Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTCXK5YCRYUG"
   },
   "source": [
    "From the above result, r-squared has improved by over 16% from the best base learner (ridge regression), while MSE has improved nearly 20%. This is a considerable improvement."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Udemy Stacking for regression Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
