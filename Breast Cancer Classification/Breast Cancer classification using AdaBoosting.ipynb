{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AU-wOg_qHtqk"
   },
   "outputs": [],
   "source": [
    "#Step 1: Import libraries and data\n",
    "from copy import  deepcopy\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "train_size = 400\n",
    "#Split the train and test set\n",
    "train_x, train_y = bc.data[:train_size], bc.target[:train_size]\n",
    "test_x, test_y = bc.data[train_size:], bc.target[train_size:]\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j09T-T1_Iu7k"
   },
   "source": [
    "We then create the ensemble.First, we declare the ensemble's size and the base learner type. We will use the decision stumps (decision trees only a single level deep).\n",
    "\n",
    "Furthermore, we create a numpy array for the data instance weights, the learners'weights and the learners' errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1_WcK8WWJEQu"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the ensemble\n",
    "ensemble_size = 3\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "# create the initial weights\n",
    "data_weights = np.zeros(train_size) + 1/train_size\n",
    "# Create a list of indices for the train set\n",
    "indices = [x for x in range(train_size)]\n",
    "base_learners = []\n",
    "learners_errors = np.zeros(ensemble_size)\n",
    "learners_weights = np.zeros(ensemble_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRjMBEjOJoAU"
   },
   "source": [
    "For each base learner, we will create a deepcopy of the original classifier , train it on the sample dataset and evaluate it.\n",
    "\n",
    "First we will create a copy and sample with replacement from the original test set according to the instance'weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lrzfJ2gCJ4NJ"
   },
   "outputs": [],
   "source": [
    "#Create each base learner\n",
    "for i in range(ensemble_size):\n",
    "  weak_learner = deepcopy(base_classifier)\n",
    "  #Choose the samples by sampling with replacement\n",
    "  #Each instace's probability is dictated by its weight\n",
    "  data_indices = np.random.choice(indices, train_size, p=data_weights)\n",
    "  sample_x, sample_y = train_x[data_indices],train_y[data_indices]\n",
    "  # Fit the weak learner and evaluatre it\n",
    "  weak_learner.fit(sample_x, sample_y)\n",
    "  predictions = weak_learner.predict(train_x)\n",
    "  errors = predictions != train_y\n",
    "  corrects = predictions == train_y\n",
    "  # calculate the wieghted errors\n",
    "  weighted_errors = data_weights*errors\n",
    "  # The base learner's error is the average of the weighted errors\n",
    "  learner_error = np.mean(weighted_errors)\n",
    "  learners_errors[i] = learner_error\n",
    "  #the learner's weight\n",
    "  learner_weight = np.log((1-learner_error)/learner_error)/2\n",
    "  learners_weights[i] = learner_weight\n",
    "  # Update the data weights\n",
    "  data_weights[errors] =  np.exp(data_weights[errors]*learner_weight)\n",
    "  data_weights[corrects] =  np.exp(-data_weights[corrects]*learner_weight)\n",
    "  data_weights = data_weights/sum(data_weights)\n",
    "  #Save the learner\n",
    "  base_learners.append(weak_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G_3mgiXKatp"
   },
   "source": [
    "We then fit the learner on the sampled dataset and predict the original train set. We use the predictions to see which instances are correctly classified and which instances are misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGQ5ND-pLLP7"
   },
   "source": [
    "The weighted errors are classified . Both errors and corrects are lists of Booleans (True or False), but Python handles them as 1 and 0. This allows us to multiply element-wise with data_weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ySTGPAPL014"
   },
   "source": [
    "Finally , the learner's weight can be calcuated as half the natural logarithm of the weighted accuracy over the weighted error. In turn, we can use the learner's weight to calcuate the enw data weights.\n",
    "\n",
    "\n",
    "For erroneously classified instances, the new weigth equals to the natural exponent of the old weight times the learner's weight.\n",
    "\n",
    "For correct classified instance, the negative multiple is used instead.\n",
    "\n",
    "Finally the new weights are normalized and the base learner is added to the base_learners list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy1ViUOaNGYF"
   },
   "source": [
    "In order to make predictions with the ensemble, we combine each individual predictions throguh a weighted majority voting. As this is a binary classificaion problem, if the weighted average is more than 0.5, the instance is classified as 0; otherwise, it's classified as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rL_LYBllKpBZ"
   },
   "outputs": [],
   "source": [
    "# Step 3: Evaluate the ensemble\n",
    "ensemble_predictions = []\n",
    "for learner, weight in zip(base_learners, learners_weights):\n",
    "    # Calculate the weighted predictions\n",
    "    prediction = learner.predict(test_x)\n",
    "    ensemble_predictions.append(prediction*weight)\n",
    "\n",
    "# The final prediction is the weighted mean of the individual predictions\n",
    "ensemble_predictions = np.mean(ensemble_predictions, axis=0) >= 0.5\n",
    "\n",
    "ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv4rxN-zOOCy",
    "outputId": "47f2e077-4d3c-42a4-ac83-5c4660e54a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Print the accuracy\n",
    "print('Boosting: %.2f' % ensemble_acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Udemy AdaBoosting Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
