{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wri73KCoDXWi"
   },
   "source": [
    "Stacking is a viable method for both regression and classification. In this video we will use it to classify the breast cancer dataset. Again, we will use three base learners. A 5-neighbor k-NN, a decision tree limited to a max depth of 4 and a simple neural network with 1 hidden layer of 100 neurons.\n",
    "\n",
    "For the meta learner, we will use a simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lIk1r9lID2Yq"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import libraries and data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "bc = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C1lzchreC1Mn"
   },
   "outputs": [],
   "source": [
    "# Split train and test set\n",
    "train_x, train_y = bc.data[:400], bc.target[:400]\n",
    "test_x, test_y = bc.data[400:], bc.target[400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnBa0YtbEqil"
   },
   "source": [
    "We did instantiate the base learners and the meta-learner. Note that MLPCLassifier has a hidden_layer_sizes = (100,) parameter, which specifies the number of neurons for each hidden layer. Here we will have a single layer of 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UteIlTOCE6-x"
   },
   "outputs": [],
   "source": [
    "# Step 2: Create the ensemble's base learners and meta learner\n",
    "# Append the base learners to a list for ease of access\n",
    "base_learners = []\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "base_learners.append(knn)\n",
    "\n",
    "dtr = DecisionTreeClassifier(max_depth=4, random_state=123456)\n",
    "base_learners.append(dtr)\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes =(100, ), solver='lbfgs', random_state=123456)\n",
    "base_learners.append(mlpc)\n",
    "\n",
    "\n",
    "meta_learner = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxy1i5WRFmSk"
   },
   "source": [
    "Again using KFolds, we split the train set into 5 folds in order to train on four folds and generate metadata for the remaining fold, repeated 5 times.\n",
    "\n",
    "Note that we use learner.predict_proba(train_x[test_indices])[:,0] in order to get the predicted probability that the instance belongs to in the first class. Given that we have only 2 classes, this is sufficient. For N classes, we would have to either save N-1 features or simply use learner.predict, in order to save the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0Tn8MB6zGKk-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: create the training metadata\n",
    "# Create variables to store metadata and their targets\n",
    "meta_data = np.zeros((len(base_learners), len(train_x)))\n",
    "meta_targets = np.zeros(len(train_x))\n",
    "\n",
    "\n",
    "# Create the cross-validation folds\n",
    "KF = KFold(n_splits=5)\n",
    "meta_index = 0\n",
    "for train_indices, test_indices in KF.split(train_x):\n",
    "    # Train each learner on the K-1 folds and create meta data for the Kth fold\n",
    "    for i in range(len(base_learners)):\n",
    "        learner = base_learners[i]\n",
    "\n",
    "        learner.fit(train_x[train_indices], train_y[train_indices])\n",
    "        predictions = learner.predict_proba(train_x[test_indices])[:,0]\n",
    "\n",
    "        meta_data[i][meta_index:meta_index+len(test_indices)] = predictions\n",
    "\n",
    "    meta_targets[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n",
    "    meta_index += len(test_indices)\n",
    "\n",
    "# Transpose the metadata to be fed into the meta-learner\n",
    "meta_data = meta_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mja4TKpyJ2Uu"
   },
   "source": [
    "Then we will train the base classifiers on the train set and create metadata for the test set as well as evaluating their accuracy with metrics.accruracy_score(test_y, leaner.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0if0BsB1KF81"
   },
   "outputs": [],
   "source": [
    "# Step 4: create the metadata for the test set and evaluate the base learners\n",
    "test_meta_data = np.zeros((len(base_learners), len(test_x)))\n",
    "base_acc = []\n",
    "for i in range(len(base_learners)):\n",
    "    learner = base_learners[i]\n",
    "    learner.fit(train_x, train_y)\n",
    "    predictions = learner.predict_proba(test_x)[:,0]\n",
    "    test_meta_data[i] = predictions\n",
    "\n",
    "    acc = metrics.accuracy_score(test_y, learner.predict(test_x))\n",
    "\n",
    "\n",
    "    base_acc.append(acc)\n",
    "\n",
    "test_meta_data = test_meta_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59NRCnmHK2ae"
   },
   "source": [
    "Finally we fit the meta-learner on the train metadata, evaluate its performance on the test data and print both the ensemble's and the individual learner's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RcyKe9AqLDKE"
   },
   "outputs": [],
   "source": [
    "# Step 5: fit the meta-learner on the train set and evaluate it on the test set\n",
    "meta_learner.fit(meta_data, meta_targets)\n",
    "ensemble_predictions = meta_learner.predict(test_meta_data)\n",
    "\n",
    "acc = metrics.accuracy_score(test_y, ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNVKXsAQLXVm",
    "outputId": "6ab21d3a-3412-4413-d92f-f7c32dd45f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc  Name\n",
      "--------------------\n",
      "0.86 KNeighborsClassifier\n",
      "0.88 DecisionTreeClassifier\n",
      "0.90 MLPClassifier\n",
      "0.93 Ensemble\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Print the result\n",
    "print('Acc  Name')\n",
    "print('-'*20)\n",
    "for i in range(len(base_learners)):\n",
    "    learner = base_learners[i]\n",
    "\n",
    "    print(f'{base_acc[i]:.2f} {learner.__class__.__name__}')\n",
    "print(f'{acc:.2f} Ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPLJFomCNwFc"
   },
   "source": [
    "Here we can see that meta-learner helps us to reach 91% accuracy of the performance and it out-perform all the base learner."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Udemy Stacking for classification Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
